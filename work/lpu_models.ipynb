{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee87f985-06ba-438e-a73d-f22c842c9fcb",
   "metadata": {},
   "source": [
    "# CloudRIC Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a409c7c-9a2a-488a-b751-c05310bb35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @author:\n",
    "        - Leonardo Lo Schiavo\n",
    "    @affiliation:\n",
    "        - IMDEA Networks institute\n",
    "'''\n",
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81146d-b932-4b9a-a0f8-961f01b9b763",
   "metadata": {},
   "source": [
    "## PyTorch Models for LPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d89ca4-63c0-4cf6-9c60-e2d92171912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @author:\n",
    "        - Leonardo Lo Schiavo\n",
    "    @affiliation:\n",
    "        - IMDEA Networks institute\n",
    "'''\n",
    "class Predictor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, nonlin=F.relu, norm_in=False):\n",
    "\n",
    "        super(Predictor, self).__init__()\n",
    "\n",
    "        if norm_in:\n",
    "            self.in_fn = nn.BatchNorm1d(input_size, affine=False)\n",
    "        else:\n",
    "            self.in_fn = lambda x: x\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        inp = self.in_fn(X)\n",
    "        h1 = self.nonlin(self.fc1(inp))\n",
    "        h2 = self.nonlin(self.fc2(h1))\n",
    "        out = self.fc3(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d45b61-54fe-4cb5-aeda-c8368e6ce00f",
   "metadata": {},
   "source": [
    "### Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff2ab8-1b00-42c7-a264-82ce32fe8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @author:\n",
    "        - Leonardo Lo Schiavo\n",
    "    @affiliation:\n",
    "        - IMDEA Networks institute\n",
    "'''\n",
    "class LPUModels:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.max_snr = 30.0\n",
    "        self.max_mcs = 27\n",
    "        self.max_total_bits = 295680\n",
    "        self.max_prbs = 250\n",
    "\n",
    "        # LPU models parameters\n",
    "        self.input_size = 3\n",
    "        self.output_size = 1\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        # Import max and min values for the inputs (only GPU power model)\n",
    "        self.max_gpu_power = np.load('/home/jovyan/data/max_power_gpu.npy')\n",
    "        self.min_gpu_power = np.load('/home/jovyan/data/min_power_gpu.npy')\n",
    "\n",
    "        # Load the model weights and set the model to inference mode\n",
    "        self.predictor_time_cpu = Predictor(self.input_size, self.output_size, self.hidden_size)\n",
    "        self.predictor_time_cpu.load_state_dict(torch.load('/home/jovyan/data/predictor_time_cpu.pyt', map_location=\"cpu\"))\n",
    "        self.predictor_time_cpu.eval()\n",
    "        self.predictor_time_gpu = Predictor(self.input_size, self.output_size, self.hidden_size)\n",
    "        self.predictor_time_gpu.load_state_dict(torch.load('/home/jovyan/data/predictor_time_gpu.pyt', map_location=\"cpu\"))\n",
    "        self.predictor_time_gpu.eval()\n",
    "        self.predictor_power_cpu = Predictor(self.input_size, self.output_size, self.hidden_size)\n",
    "        self.predictor_power_cpu.load_state_dict(torch.load('/home/jovyan/data/predictor_power_cpu.pyt', map_location=\"cpu\"))\n",
    "        self.predictor_power_cpu.eval()\n",
    "        self.predictor_power_gpu = Predictor(self.input_size, self.output_size, self.hidden_size)\n",
    "        self.predictor_power_gpu.load_state_dict(torch.load('/home/jovyan/data/predictor_power_gpu.pyt', map_location=\"cpu\"))\n",
    "        self.predictor_power_gpu.eval()\n",
    "\n",
    "\n",
    "    def estimate_service_time(self, snr, mcs, prbs, total_bits):\n",
    "        # Normalize the inputs and format them for PyTorch\n",
    "        power_inputs = []\n",
    "        power_inputs.append(snr / self.max_snr)\n",
    "        power_inputs.append(mcs / self.max_mcs)\n",
    "        power_inputs.append(prbs / self.max_prbs)\n",
    "        power_inputs = torch.Tensor(power_inputs)\n",
    "\n",
    "        time_inputs = []\n",
    "        time_inputs.append(snr / self.max_snr)\n",
    "        time_inputs.append(mcs / self.max_mcs)\n",
    "        time_inputs.append(total_bits / self.max_total_bits)\n",
    "        time_inputs = torch.Tensor(time_inputs)\n",
    "        # Run the model in inference mode\n",
    "        power_cpu = self.predictor_power_cpu(power_inputs).detach().numpy()[0]\n",
    "        time_cpu = float(self.predictor_time_cpu(time_inputs).detach().numpy()[0])\n",
    "        energy_cpu = float(power_cpu * time_cpu)\n",
    "        power_gpu = self.predictor_power_gpu(power_inputs).detach().numpy()[0] * (self.max_gpu_power - self.min_gpu_power) + self.min_gpu_power\n",
    "        time_gpu = float(self.predictor_time_gpu(time_inputs).detach().numpy()[0])\n",
    "        energy_gpu = float(power_gpu * time_gpu)\n",
    "\n",
    "        return time_cpu, energy_cpu, time_gpu, energy_gpu\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f595c0-f0a8-4468-8592-c954ff2d6a19",
   "metadata": {},
   "source": [
    "### Predict Time and Power for the LPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa043955-8785-4ca7-9d3f-1c0f94462056",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "    @author:\n",
    "        - Leonardo Lo Schiavo\n",
    "    @affiliation:\n",
    "        - IMDEA Networks institute\n",
    "'''\n",
    "def estimate_service_time(row):\n",
    "    t_cpu, e_cpu, t_gpu, e_gpu = lpu_models.estimate_service_time(row['SNR'], row['MCS'], row['PRBs'], row['TBS'])\n",
    "    return t_cpu, e_cpu, t_gpu, e_gpu\n",
    "\n",
    "input_file = \"/home/jovyan/data/trace_40BS.csv\"\n",
    "\n",
    "# Load Trace\n",
    "trace_df = pd.read_csv(input_file, sep=\",\", header=0,nrows=1000)\n",
    "trace_df.value_counts()\n",
    "\n",
    "# Create an instance of LPU Models\n",
    "lpu_models = LPUModels()\n",
    "\n",
    "# Run inference for each request within the trace\n",
    "results_df = pd.DataFrame(columns = ['t_cpu', 'e_cpu', 't_gpu', 'e_gpu'])\n",
    "results_df['t_cpu'], results_df['e_cpu'],results_df['t_gpu'],results_df['e_gpu'] = zip(*trace_df.apply(estimate_service_time, axis=1))\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f0b2d-a4a5-454e-aad8-d1d29c3ec136",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62115c-b7dc-42b7-ac08-55099aec7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpu_models = LPUModels()\n",
    "torch_input = torch.randn(lpu_models.input_size)\n",
    "onnx_program = torch.onnx.dynamo_export(lpu_models.predictor_time_cpu, torch_input)\n",
    "onnx_program.save(\"/home/jovyan/data/predictor_time_cpu.onnx\")\n",
    "onnx_program = torch.onnx.dynamo_export(lpu_models.predictor_time_gpu, torch_input)\n",
    "onnx_program.save(\"/home/jovyan/data/predictor_time_gpu.onnx\")\n",
    "torch_input = torch.randn(lpu_models.input_size)\n",
    "onnx_program = torch.onnx.dynamo_export(lpu_models.predictor_time_cpu, torch_input)\n",
    "onnx_program.save(\"/home/jovyan/data/predictor_power_cpu.onnx\")\n",
    "onnx_program = torch.onnx.dynamo_export(lpu_models.predictor_time_gpu, torch_input)\n",
    "onnx_program.save(\"/home/jovyan/data/predictor_power_gpu.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7757aa1-2620-4f24-b901-f6d1d501e3cc",
   "metadata": {},
   "source": [
    "## Perform Inference using ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0cf89-1560-47d5-8d38-6f10069a635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @author:\n",
    "        - Leonardo Lo Schiavo\n",
    "    @affiliation:\n",
    "        - IMDEA Networks institute\n",
    "'''\n",
    "import onnxruntime\n",
    "class LPUModelsONNX:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Dataset parameters\n",
    "        self.max_snr = 30.0\n",
    "        self.max_mcs = 27\n",
    "        self.max_total_bits = 295680\n",
    "        self.max_prbs = 250\n",
    "\n",
    "        # LPU models parameters\n",
    "        self.input_size = 3\n",
    "        self.output_size = 1\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        # Import max and min values for the inputs (only GPU power model)\n",
    "        self.max_gpu_power = np.load('/home/jovyan/data/max_power_gpu.npy')\n",
    "        self.min_gpu_power = np.load('/home/jovyan/data/min_power_gpu.npy')\n",
    "\n",
    "        # Load the model weights and set the model to inference mode\n",
    "        self.predictor_time_cpu = onnxruntime.InferenceSession(\"/home/jovyan/data/predictor_time_cpu.onnx\", providers=['CPUExecutionProvider']) \n",
    "        self.predictor_time_gpu = onnxruntime.InferenceSession(\"/home/jovyan/data/predictor_time_gpu.onnx\", providers=['CPUExecutionProvider']) \n",
    "        self.predictor_power_cpu = onnxruntime.InferenceSession(\"/home/jovyan/data/predictor_power_cpu.onnx\", providers=['CPUExecutionProvider']) \n",
    "        self.predictor_power_gpu = onnxruntime.InferenceSession(\"/home/jovyan/data/predictor_power_gpu.onnx\", providers=['CPUExecutionProvider']) \n",
    "        \n",
    "    def to_numpy(self,tensor):\n",
    "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "    def run_onnx(self,model,onnx_input):\n",
    "        onnxruntime_input = {k.name: self.to_numpy(v) for k, v in zip(model.get_inputs(), onnx_input)}\n",
    "        onnxruntime_outputs = model.run(None, onnxruntime_input)\n",
    "        return onnxruntime_outputs[0]\n",
    "    \n",
    "    def estimate_service_time(self, snr, mcs, prbs, total_bits):\n",
    "        # Normalize the inputs and format them for PyTorch and ONNX\n",
    "        power_inputs = []\n",
    "        power_inputs.append(snr / self.max_snr)\n",
    "        power_inputs.append(mcs / self.max_mcs)\n",
    "        power_inputs.append(prbs / self.max_prbs)\n",
    "        power_inputs = torch.Tensor(power_inputs)\n",
    "        onnx_power_inputs = onnx_program.adapt_torch_inputs_to_onnx(power_inputs)\n",
    "        \n",
    "        time_inputs = []\n",
    "        time_inputs.append(snr / self.max_snr)\n",
    "        time_inputs.append(mcs / self.max_mcs)\n",
    "        time_inputs.append(total_bits / self.max_total_bits)\n",
    "        time_inputs = torch.Tensor(time_inputs)\n",
    "        onnx_time_inputs = onnx_program.adapt_torch_inputs_to_onnx(time_inputs)\n",
    "        # Run the model in inference mode\n",
    "        power_cpu = self.run_onnx(self.predictor_power_cpu,onnx_power_inputs)[0]\n",
    "        time_cpu = self.run_onnx(self.predictor_time_cpu,onnx_time_inputs)[0]\n",
    "        power_gpu = self.run_onnx(self.predictor_power_gpu,onnx_power_inputs)[0]*(self.max_gpu_power - self.min_gpu_power) + self.min_gpu_power\n",
    "        time_gpu = self.run_onnx(self.predictor_time_gpu,onnx_time_inputs)[0]\n",
    "        energy_cpu = float(power_cpu * time_cpu)\n",
    "        energy_gpu = float(power_gpu * time_gpu)\n",
    "        return time_cpu, energy_cpu, time_gpu, energy_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d26da-81f1-428f-b439-0b50208122e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "    @author:\n",
    "        - Leonardo Lo Schiavo\n",
    "    @affiliation:\n",
    "        - IMDEA Networks institute\n",
    "'''\n",
    "def estimate_service_time(row):\n",
    "    t_cpu, e_cpu, t_gpu, e_gpu = lpu_models.estimate_service_time(row['SNR'], row['MCS'], row['PRBs'], row['TBS'])\n",
    "    return t_cpu, e_cpu, t_gpu, e_gpu\n",
    "\n",
    "input_file = \"/home/jovyan/data/trace_40BS.csv\"\n",
    "\n",
    "# Load Trace\n",
    "trace_df = pd.read_csv(input_file, sep=\",\", header=0,nrows=1000)\n",
    "trace_df.value_counts()\n",
    "\n",
    "# Create an instance of LPU Models\n",
    "lpu_models = LPUModelsONNX()\n",
    "\n",
    "# Run inference for each request within the trace\n",
    "results_df = pd.DataFrame(columns = ['t_cpu', 'e_cpu', 't_gpu', 'e_gpu'])\n",
    "results_df['t_cpu'], results_df['e_cpu'],results_df['t_gpu'],results_df['e_gpu'] = zip(*trace_df.apply(estimate_service_time, axis=1))\n",
    "\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
